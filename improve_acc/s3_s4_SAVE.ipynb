{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from catboost import CatBoostClassifier\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2570)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_3_samples = pd.read_csv('../data/cancer/stage_3_prostate_cancer_samples.csv')\n",
    "stage_3_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 2570)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_4_samples = pd.read_csv('../data/cancer/stage_4_prostate_cancer_samples.csv')\n",
    "stage_4_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = pd.concat([stage_3_samples, stage_4_samples], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Stage column: ['Stage: 3' 'Stage: 4']\n",
      "Class distribution in ID_REF column:\n",
      "ID_REF\n",
      "0    150\n",
      "1     91\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify and clean the Stage column\n",
    "print(\"Unique values in Stage column:\", combined_dataset['Stage'].unique())\n",
    "combined_dataset['Stage'] = combined_dataset['Stage'].str.strip()\n",
    "combined_dataset['ID_REF'] = np.where(combined_dataset['Stage'] == 'Stage: 3', 0, 1)\n",
    "combined_dataset['ID_REF'] = np.where(combined_dataset['Stage'] == 'Stage: 4', 1, combined_dataset['ID_REF'])\n",
    "\n",
    "# Print class distribution to ensure both classes are present\n",
    "print(\"Class distribution in ID_REF column:\")\n",
    "print(combined_dataset['ID_REF'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define process_data function\n",
    "def process_data(data, under_sample_factor=None, over_sample_factor=None):\n",
    "    columns_to_drop = ['Sample_ID', 'Sex', 'Age', 'Stage', 'Disease']\n",
    "    data = data.drop(columns=columns_to_drop, axis=1)\n",
    "    \n",
    "    x = np.array(data.drop([\"ID_REF\"], axis=1)).astype('float')\n",
    "    y = np.array(data[\"ID_REF\"]).astype('int')\n",
    "    feature_names = data.columns[1:]\n",
    "\n",
    "    if under_sample_factor is not None and isinstance(under_sample_factor, float) and 0 < under_sample_factor <= 1:\n",
    "        under_sampler = RandomUnderSampler(sampling_strategy=under_sample_factor)\n",
    "        x, y = under_sampler.fit_resample(x, y)\n",
    "\n",
    "    if over_sample_factor is not None and isinstance(over_sample_factor, float) and 0 < over_sample_factor <= 1:\n",
    "        over_sampler = RandomOverSampler(sampling_strategy=over_sample_factor)\n",
    "        x, y = over_sampler.fit_resample(x, y)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "feature_selection_num = 500\n",
    "feature_importance_num = 20\n",
    "pca_components = 100  # Number of principal components for PCA\n",
    "\n",
    "# Process data\n",
    "x_train, x_test, y_train, y_test, feature_names = process_data(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna - SVM\n",
    "def svm_objective(trial):\n",
    "    k = feature_selection_num\n",
    "    \n",
    "    C = trial.suggest_loguniform('C', 1e-3, 1e3)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('skb', SelectKBest(f_classif, k=k)),\n",
    "        ('pca', PCA(n_components=pca_components)),\n",
    "        ('estimator', SVC(C=C, kernel=kernel, random_state=0))\n",
    "    ])\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(pipe, x_train, y_train, cv=cv, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_objective(trial):\n",
    "    k = feature_selection_num\n",
    "    \n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('skb', SelectKBest(f_classif, k=k)),\n",
    "        ('pca', PCA(n_components=pca_components)),\n",
    "        ('estimator', RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, criterion=criterion, random_state=0))\n",
    "    ])\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(pipe, x_train, y_train, cv=cv, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_objective(trial):\n",
    "    k = feature_selection_num\n",
    "    \n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-3, 1.0)\n",
    "    depth = trial.suggest_int('depth', 2, 10)\n",
    "    iterations = trial.suggest_int('iterations', 100, 1000)\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('skb', SelectKBest(f_classif, k=k)),\n",
    "        ('pca', PCA(n_components=pca_components)),\n",
    "        ('estimator', CatBoostClassifier(learning_rate=learning_rate, depth=depth, iterations=iterations, verbose=0, random_state=0))\n",
    "    ])\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(pipe, x_train, y_train, cv=cv, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save study\n",
    "def save_study(study, filename):\n",
    "    joblib.dump(study, filename)\n",
    "\n",
    "# Function to load study\n",
    "def load_study(filename):\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize hyperparameters using Optuna\n",
    "svm_study_filename = 'svm_study_s3_s4.pkl'\n",
    "rf_study_filename = 'rf_study_s3_s4.pkl'\n",
    "catboost_study_filename = 'catboost_study_s3_s4.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Hyperparmeters if not trained already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Hyperparameter Optimization\n",
    "if os.path.exists(svm_study_filename):\n",
    "    svm_study = load_study(svm_study_filename)\n",
    "else:\n",
    "    svm_study = optuna.create_study(direction='maximize')\n",
    "    svm_study.optimize(svm_objective, n_trials=50)\n",
    "    save_study(svm_study, svm_study_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF Hyperparameter Optimization\n",
    "if os.path.exists(rf_study_filename):\n",
    "    rf_study = load_study(rf_study_filename)\n",
    "else:\n",
    "    rf_study = optuna.create_study(direction='maximize')\n",
    "    rf_study.optimize(rf_objective, n_trials=50)\n",
    "    save_study(rf_study, rf_study_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Hyperparameter Optimization\n",
    "if os.path.exists(catboost_study_filename):\n",
    "    catboost_study = load_study(catboost_study_filename)\n",
    "else:\n",
    "    catboost_study = optuna.create_study(direction='maximize')\n",
    "    catboost_study.optimize(catboost_objective, n_trials=50)\n",
    "    save_study(catboost_study, catboost_study_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the best trial for each study\n",
    "\n",
    "For SVM, RF, and Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM trial:\n",
      "  Value:  0.6353576248313091\n",
      "  Params: \n",
      "    C: 1.8492946891319226\n",
      "    kernel: poly\n"
     ]
    }
   ],
   "source": [
    "print(\"Best SVM trial:\")\n",
    "svm_trial = svm_study.best_trial\n",
    "print(\"  Value: \", svm_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in svm_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest trial:\n",
      "  Value:  0.6354925775978407\n",
      "  Params: \n",
      "    n_estimators: 585\n",
      "    max_depth: 14\n",
      "    max_features: sqrt\n",
      "    criterion: entropy\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Random Forest trial:\")\n",
    "rf_trial = rf_study.best_trial\n",
    "print(\"  Value: \", rf_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in rf_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CatBoost trial:\n",
      "  Value:  0.6356275303643726\n",
      "  Params: \n",
      "    learning_rate: 0.01462911525171483\n",
      "    depth: 10\n",
      "    iterations: 368\n"
     ]
    }
   ],
   "source": [
    "print(\"Best CatBoost trial:\")\n",
    "catboost_trial = catboost_study.best_trial\n",
    "print(\"  Value: \", catboost_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in catboost_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the models with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the models with the best hyperparameters\n",
    "def train_and_evaluate(pipe, x_train, y_train, x_test, y_test):\n",
    "    pipe.fit(x_train, y_train)\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    print(f'Testing accuracy {accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Confusion matrix: \\n{confusion_matrix(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.5918367346938775\n",
      "Confusion matrix: \n",
      "[[29  1]\n",
      " [19  0]]\n"
     ]
    }
   ],
   "source": [
    "best_svm_params = svm_trial.params\n",
    "svm_pipe = Pipeline([\n",
    "    ('skb', SelectKBest(f_classif, k=feature_selection_num)),\n",
    "    ('pca', PCA(n_components=pca_components)),\n",
    "    ('estimator', SVC(C=best_svm_params['C'], kernel=best_svm_params['kernel'], random_state=0))\n",
    "])\n",
    "train_and_evaluate(svm_pipe, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.6530612244897959\n",
      "Confusion matrix: \n",
      "[[30  0]\n",
      " [17  2]]\n"
     ]
    }
   ],
   "source": [
    "best_rf_params = rf_trial.params\n",
    "rf_pipe = Pipeline([\n",
    "    ('skb', SelectKBest(f_classif, k=feature_selection_num)),\n",
    "    ('pca', PCA(n_components=pca_components)),\n",
    "    ('estimator', RandomForestClassifier(n_estimators=best_rf_params['n_estimators'],\n",
    "                                         max_depth=best_rf_params['max_depth'],\n",
    "                                         max_features=best_rf_params['max_features'],\n",
    "                                         criterion=best_rf_params['criterion'],\n",
    "                                         random_state=0))\n",
    "])\n",
    "train_and_evaluate(rf_pipe, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.5918367346938775\n",
      "Confusion matrix: \n",
      "[[28  2]\n",
      " [18  1]]\n"
     ]
    }
   ],
   "source": [
    "best_catboost_params = catboost_trial.params\n",
    "catboost_pipe = Pipeline([\n",
    "    ('skb', SelectKBest(f_classif, k=feature_selection_num)),\n",
    "    ('pca', PCA(n_components=pca_components)),\n",
    "    ('estimator', CatBoostClassifier(learning_rate=best_catboost_params['learning_rate'],\n",
    "                                     depth=best_catboost_params['depth'],\n",
    "                                     iterations=best_catboost_params['iterations'],\n",
    "                                     verbose=0,\n",
    "                                     random_state=0))\n",
    "])\n",
    "train_and_evaluate(catboost_pipe, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance and top features can be extracted similarly to the previous script\n",
    "def get_top_features(pipe, feature_names, top_feature_num):\n",
    "    if isinstance(pipe.named_steps['estimator'], SVC):\n",
    "        if pipe.named_steps['estimator'].kernel != 'linear':\n",
    "            raise ValueError(\"Feature importance is not available for non-linear SVM kernels.\")\n",
    "        feature_scores = pipe.named_steps['estimator'].coef_[0]\n",
    "    elif isinstance(pipe.named_steps['estimator'], RandomForestClassifier) or isinstance(pipe.named_steps['estimator'], CatBoostClassifier):\n",
    "        feature_scores = pipe.named_steps['estimator'].feature_importances_\n",
    "    features = pipe.named_steps['skb'].get_support(indices=True)\n",
    "    top_indices = np.argsort(np.abs(feature_scores))[::-1][:top_feature_num]\n",
    "    top_features = [(feature_names[i], feature_scores[i]) for i in top_indices]\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM feature extraction error: Feature importance is not available for non-linear SVM kernels.\n",
      "Top Random Forest features: [('hsa-miR-106a-3p', 0.03830524499573508), ('hsa-miR-548l', 0.03151128579083724), ('hsa-miR-124-5p', 0.031072531952670023), ('hsa-miR-512-3p', 0.027539310522499534), ('hsa-miR-1207-3p', 0.024001600606678185), ('hsa-miR-27a-5p', 0.02247050653083074), ('hsa-miR-498', 0.018352545805131026), ('hsa-miR-6829-3p', 0.01623961783500971), ('hsa-miR-4680-5p', 0.015200772536816948), ('hsa-miR-490-3p', 0.014342863051951774), ('hsa-miR-5583-3p', 0.013941718885508962), ('hsa-miR-3124-3p', 0.013474662817620052), ('hsa-miR-4802-3p', 0.012682279240234155), ('hsa-miR-6716-3p', 0.012332769282919613), ('hsa-miR-1254', 0.011906986089753118), ('hsa-miR-1208', 0.011800972090224989), ('hsa-miR-3973', 0.011605988377774795), ('hsa-miR-6717-5p', 0.011418844437422512), ('hsa-miR-4722-3p', 0.011277881685529609), ('hsa-miR-196a-3p', 0.011081294595093476)]\n",
      "Top CatBoost features: [('hsa-miR-106a-3p', 5.633262403672353), ('hsa-miR-124-5p', 4.381795164544335), ('hsa-miR-548l', 3.1967185999487437), ('hsa-miR-1207-3p', 2.4331836007077783), ('hsa-miR-512-3p', 2.3929286191212125), ('hsa-miR-490-3p', 1.9499987858089833), ('hsa-miR-498', 1.7608434520859066), ('hsa-miR-27a-5p', 1.7602897957870973), ('hsa-miR-4680-5p', 1.5181349825387822), ('hsa-miR-6717-5p', 1.3751196797156866), ('hsa-miR-518e-3p', 1.3335509870876905), ('hsa-miR-4281', 1.3153596822520328), ('hsa-miR-1255a', 1.2757042814700188), ('hsa-miR-4483', 1.2650912798248628), ('hsa-miR-4285', 1.2086871101327263), ('hsa-miR-4802-3p', 1.1789430000616345), ('hsa-miR-2277-3p', 1.1513960709753142), ('hsa-miR-451a', 1.1351661631543464), ('hsa-miR-8072', 1.1134985435955145), ('hsa-miR-3140-5p', 1.0885171899128976)]\n"
     ]
    }
   ],
   "source": [
    "# Get top features for each model\n",
    "try:\n",
    "    svm_top_features = get_top_features(svm_pipe, feature_names, feature_importance_num)\n",
    "    print(\"Top SVM features:\", svm_top_features)\n",
    "except ValueError as e:\n",
    "    print(\"SVM feature extraction error:\", e)\n",
    "\n",
    "rf_top_features = get_top_features(rf_pipe, feature_names, feature_importance_num)\n",
    "catboost_top_features = get_top_features(catboost_pipe, feature_names, feature_importance_num)\n",
    "\n",
    "print(\"Top Random Forest features:\", rf_top_features)\n",
    "print(\"Top CatBoost features:\", catboost_top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsa-miR-106a-3p\n",
      "hsa-miR-548l\n",
      "hsa-miR-124-5p\n",
      "hsa-miR-512-3p\n",
      "hsa-miR-1207-3p\n",
      "hsa-miR-27a-5p\n",
      "hsa-miR-498\n",
      "hsa-miR-6829-3p\n",
      "hsa-miR-4680-5p\n",
      "hsa-miR-490-3p\n",
      "hsa-miR-5583-3p\n",
      "hsa-miR-3124-3p\n",
      "hsa-miR-4802-3p\n",
      "hsa-miR-6716-3p\n",
      "hsa-miR-1254\n",
      "hsa-miR-1208\n",
      "hsa-miR-3973\n",
      "hsa-miR-6717-5p\n",
      "hsa-miR-4722-3p\n",
      "hsa-miR-196a-3p\n",
      "hsa-miR-106a-3p\n",
      "hsa-miR-124-5p\n",
      "hsa-miR-548l\n",
      "hsa-miR-1207-3p\n",
      "hsa-miR-512-3p\n",
      "hsa-miR-490-3p\n",
      "hsa-miR-498\n",
      "hsa-miR-27a-5p\n",
      "hsa-miR-4680-5p\n",
      "hsa-miR-6717-5p\n",
      "hsa-miR-518e-3p\n",
      "hsa-miR-4281\n",
      "hsa-miR-1255a\n",
      "hsa-miR-4483\n",
      "hsa-miR-4285\n",
      "hsa-miR-4802-3p\n",
      "hsa-miR-2277-3p\n",
      "hsa-miR-451a\n",
      "hsa-miR-8072\n",
      "hsa-miR-3140-5p\n"
     ]
    }
   ],
   "source": [
    "# Compile and print the list of top features\n",
    "def compile_top_features_list(rf_features, cb_features):\n",
    "    top_features = rf_features + cb_features\n",
    "    feature_names = [feature[0] for feature in top_features]\n",
    "    return '\\n'.join(feature_names)\n",
    "\n",
    "top_features_list = compile_top_features_list(rf_top_features, catboost_top_features)\n",
    "print(top_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save top features list to a file using pandas\n",
    "top_features_df = pd.DataFrame(top_features_list.split('\\n'), columns=['Feature'])\n",
    "top_features_df.to_csv('../GSEA/miRNA/s3_s4_miRNA.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
