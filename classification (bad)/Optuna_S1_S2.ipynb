{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancer data\n",
    "stage_1_samples = pd.read_csv('../data/cancer/stage_1_prostate_cancer_samples.csv')\n",
    "stage_1_samples.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_2_samples = pd.read_csv('../data/cancer/stage_2_prostate_cancer_samples.csv')\n",
    "stage_2_samples.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine stage 1 and stage 2 samples\n",
    "combined_dataset = pd.concat([stage_1_samples, stage_2_samples], ignore_index=True)\n",
    "\n",
    "# Verify the Stage column exists and print unique values\n",
    "print(\"Unique values in Stage column:\", combined_dataset['Stage'].unique())\n",
    "\n",
    "# Strip the \"Stage: \" prefix from the Stage column and then label the samples\n",
    "combined_dataset['Stage'] = combined_dataset['Stage'].str.strip()\n",
    "\n",
    "# Label the samples (0 for stage 1, 1 for stage 2)\n",
    "combined_dataset['ID_REF'] = np.where(combined_dataset['Stage'] == 'Stage: 1', 0, 1)\n",
    "combined_dataset['ID_REF'] = np.where(combined_dataset['Stage'] == 'Stage: 2', 1, combined_dataset['ID_REF'])\n",
    "\n",
    "# Print class distribution to ensure both classes are present\n",
    "print(\"Class distribution in ID_REF column:\")\n",
    "print(combined_dataset['ID_REF'].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define process_data function\n",
    "def process_data(data, under_sample_factor=None, over_sample_factor=None):\n",
    "    # Drop metadata columns\n",
    "    columns_to_drop = ['Sample_ID', 'Sex', 'Age', 'Stage', 'Disease']\n",
    "    data = data.drop(columns=columns_to_drop, axis=1)\n",
    "    \n",
    "    x = np.array(data.drop([\"ID_REF\"], axis=1)).astype('float')\n",
    "    y = np.array(data[\"ID_REF\"]).astype('int')\n",
    "    feature_names = data.columns[1:]\n",
    "\n",
    "    if under_sample_factor is not None and isinstance(under_sample_factor, float) and 0 < under_sample_factor <= 1:\n",
    "        under_sampler = RandomUnderSampler(sampling_strategy=under_sample_factor)\n",
    "        x, y = under_sampler.fit_resample(x, y)\n",
    "\n",
    "    if over_sample_factor is not None and isinstance(over_sample_factor, float) and 0 < over_sample_factor <= 1:\n",
    "        over_sampler = RandomOverSampler(sampling_strategy=over_sample_factor)\n",
    "        x, y = over_sampler.fit_resample(x, y)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "feature_selection_num = 500\n",
    "feature_importance_num = 10\n",
    "\n",
    "# Process data\n",
    "x_train, x_test, y_train, y_test, feature_names = process_data(combined_dataset, under_sample_factor=None, over_sample_factor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna - SVM\n",
    "def svm_objective(trial):\n",
    "    k = feature_selection_num\n",
    "    \n",
    "    # Suggest hyperparameters for SVM\n",
    "    C = trial.suggest_loguniform('C', 1e-6, 1e+6)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('skb', SelectKBest(f_classif, k=k)),\n",
    "        ('estimator', SVC(C=C, kernel=kernel))\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    scores = cross_val_score(pipe, x_train, y_train, cv=cv, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna - Random Forest\n",
    "def rf_objective(trial):\n",
    "    k = feature_selection_num\n",
    "    \n",
    "    # Suggest hyperparameters for Random Forest\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('skb', SelectKBest(f_classif, k=k)),\n",
    "        ('estimator', RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, criterion=criterion))\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    scores = cross_val_score(pipe, x_train, y_train, cv=cv, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna - XGBoost\n",
    "def xgb_objective(trial):\n",
    "    k = feature_selection_num\n",
    "    \n",
    "    # Suggest hyperparameters for XGBoost\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e+1)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('skb', SelectKBest(f_classif, k=k)),\n",
    "        ('estimator', XGBClassifier(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth))\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    scores = cross_val_score(pipe, x_train, y_train, cv=cv, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize hyperparameters using Optuna\n",
    "svm_study = optuna.create_study(direction='maximize')\n",
    "svm_study.optimize(svm_objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with hyperparameter optimization\n",
    "rf_study = optuna.create_study(direction='maximize')\n",
    "rf_study.optimize(rf_objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XGBoost with hyperparameter optimization\n",
    "# xgb_study = optuna.create_study(direction='maximize')\n",
    "# xgb_study.optimize(xgb_objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best trial for each study\n",
    "print(\"Best SVM trial:\")\n",
    "svm_trial = svm_study.best_trial\n",
    "print(\"  Value: \", svm_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in svm_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"Best Random Forest trial:\")\n",
    "rf_trial = rf_study.best_trial\n",
    "print(\"  Value: \", rf_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in rf_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# print(\"Best XGBoost trial:\")\n",
    "# xgb_trial = xgb_study.best_trial\n",
    "# print(\"  Value: \", xgb_trial.value)\n",
    "# print(\"  Params: \")\n",
    "# for key, value in xgb_trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "def create_network(top_features_list, all_features_list, correlation_threshold_factor, cancer_dataset):\n",
    "    cancer_subset, control_subset = cancer_dataset[(cancer_dataset[\"ID_REF\"] == 0)], cancer_dataset[(cancer_dataset[\"ID_REF\"] == 1)]\n",
    "\n",
    "    edges = [((node1, node2), cancer_subset[node1].corr(cancer_subset[node2], method=\"pearson\")) for node1, node2 in itertools.combinations(top_features_list, 2)]\n",
    "    edges = [(node1, node2, {'weight': abs(correlation), 'sign': 1 if correlation > 0 else 0}) for (node1, node2), correlation in edges if abs(correlation) >= correlation_threshold_factor]\n",
    "\n",
    "    nodes = [(feature, {'sides': all_features_list.count(feature) + 1, \"comparison\": 1 if cancer_subset[feature].mean() >= control_subset[feature].mean() else (0 if cancer_subset[feature].mean() < control_subset[feature].mean() else 0.5)}) for feature in top_features_list]\n",
    "\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_edges_from(edges)\n",
    "\n",
    "    degrees = dict(graph.degree())\n",
    "    network_degrees_values, network_degrees_nodes = zip(*sorted(zip(degrees.values(), degrees.keys()), reverse=True))\n",
    "    print(network_degrees_nodes)\n",
    "    print(network_degrees_values)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the models with the best hyperparameters\n",
    "def train_and_evaluate(pipe, x_train, y_train, x_test, y_test):\n",
    "    pipe.fit(x_train, y_train)\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "best_svm_params = svm_trial.params\n",
    "svm_pipe = Pipeline([\n",
    "    ('skb', SelectKBest(f_classif, k=feature_selection_num)),\n",
    "    ('estimator', SVC(**best_svm_params))\n",
    "])\n",
    "train_and_evaluate(svm_pipe, x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "best_rf_params = rf_trial.params\n",
    "rf_pipe = Pipeline([\n",
    "    ('skb', SelectKBest(f_classif, k=feature_selection_num)),\n",
    "    ('estimator', RandomForestClassifier(**best_rf_params))\n",
    "])\n",
    "train_and_evaluate(rf_pipe, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XGBoost\n",
    "# best_xgb_params = xgb_trial.params\n",
    "# xgb_pipe = Pipeline([\n",
    "#     ('skb', SelectKBest(f_classif, k=feature_selection_num)),\n",
    "#     ('estimator', XGBClassifier(**best_xgb_params))\n",
    "# ])\n",
    "# train_and_evaluate(xgb_pipe, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance and top features can be extracted similarly to the previous script\n",
    "\n",
    "def get_top_features(pipe, feature_names, top_feature_num):\n",
    "    if isinstance(pipe.named_steps['estimator'], SVC):\n",
    "        feature_scores = pipe.named_steps['estimator'].coef_[0]\n",
    "    elif isinstance(pipe.named_steps['estimator'], RandomForestClassifier) or isinstance(pipe.named_steps['estimator'], XGBClassifier):\n",
    "        feature_scores = pipe.named_steps['estimator'].feature_importances_\n",
    "    features = pipe.named_steps['skb'].get_support(indices=True)\n",
    "    top_indices = np.argsort(np.abs(feature_scores))[::-1][:top_feature_num]\n",
    "    top_features = [(feature_names[i], feature_scores[i]) for i in top_indices]\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features for each model\n",
    "svm_top_features = get_top_features(svm_pipe, feature_names, feature_importance_num)\n",
    "rf_top_features = get_top_features(rf_pipe, feature_names, feature_importance_num)\n",
    "# xgb_top_features = get_top_features(xgb_pipe, feature_names, feature_importance_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top SVM features:\", svm_top_features)\n",
    "print(\"Top Random Forest features:\", rf_top_features)\n",
    "# print(\"Top XGBoost features:\", xgb_top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis and visualization\n",
    "def create_network(top_features_list, all_features_list, correlation_threshold_factor, cancer_dataset):\n",
    "    cancer_subset, control_subset = cancer_dataset[(cancer_dataset[\"ID_REF\"] == 1)], cancer_dataset[(cancer_dataset[\"ID_REF\"] == 0)]\n",
    "\n",
    "    edges = [((node1, node2), cancer_subset[node1].corr(cancer_subset[node2], method=\"pearson\")) for node1, node2 in itertools.combinations(top_features_list, 2)]\n",
    "    edges = [(node1, node2, {'weight': abs(correlation), 'sign': 1 if correlation > 0 else 0}) for (node1, node2), correlation in edges if abs(correlation) >= correlation_threshold_factor]\n",
    "\n",
    "    nodes = [(feature, {'sides': all_features_list.count(feature) + 1, \"comparison\": 1 if cancer_subset[feature].mean() >= control_subset[feature].mean() else (0 if cancer_subset[feature].mean() < control_subset[feature].mean() else 0.5)}) for feature in top_features_list]\n",
    "\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_edges_from(edges)\n",
    "\n",
    "    degrees = dict(graph.degree())\n",
    "\n",
    "    network_degrees_values, network_degrees_nodes = zip(*sorted(zip(degrees.values(), degrees.keys()), reverse=True))\n",
    "    print(network_degrees_nodes)\n",
    "    print(network_degrees_values)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_charts(top_features_list, full_dataset, path_name):\n",
    "    lung_cancer = full_dataset[(full_dataset[\"ID_REF\"] == \"Lung Cancer\")]\n",
    "    colorectal_cancer = full_dataset[(full_dataset[\"ID_REF\"] == \"Colorectal Cancer\")]\n",
    "    gastric_cancer = full_dataset[(full_dataset[\"ID_REF\"] == \"Gastric Cancer\")]\n",
    "    prostate_cancer = full_dataset[(full_dataset[\"ID_REF\"] == \"Prostate Cancer\")]\n",
    "    no_cancer = full_dataset[(full_dataset[\"ID_REF\"] == \"No Cancer\")]\n",
    "\n",
    "    cancer_dataset = [lung_cancer, colorectal_cancer, gastric_cancer, prostate_cancer, no_cancer]\n",
    "\n",
    "    plt.ioff()\n",
    "    for feature_name in top_features_list:\n",
    "        plt.figure(0).clf()\n",
    "        plt.figure(figsize=(9, 6))\n",
    "\n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "        labels = ['Lung', 'Colorectal', 'Gastric', 'Prostate', 'No Cancer']\n",
    "        means = [data[feature_name].mean() for data in cancer_dataset]\n",
    "        errors = [data[feature_name].sem() * 2 for data in cancer_dataset]\n",
    "        plt.bar(labels, means, yerr=errors, error_kw={'elinewidth': 10, 'ecolor': 'k'}, capsize=15)\n",
    "\n",
    "        plt.title(feature_name)\n",
    "        plt.ylabel('Signal Value')\n",
    "        plt.savefig(\"../bar_charts/\" + path_name + \"/\" + feature_name, dpi=200, bbox_inches='tight')\n",
    "    plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create network and bar charts for visualization\n",
    "# network_graph = create_network(top_features, list(feature_names), correlation_threshold_factor=0.5, cancer_dataset=combined_dataset)\n",
    "# create_bar_charts(top_features, combined_dataset, path_name=\"Feature_Bar_Charts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
