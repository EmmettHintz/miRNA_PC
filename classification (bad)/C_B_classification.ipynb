{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancer data\n",
    "cancer_samples = pd.read_csv('../data/cancer/prostate_cancer_samples.csv')\n",
    "cancer_samples.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_samples = pd.read_csv('../data/benign/benign_prostate_samples.csv')\n",
    "print(benign_samples.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine cancer and non-cancer samples\n",
    "combined_dataset = pd.concat([benign_samples, cancer_samples], ignore_index=True)\n",
    "\n",
    "\n",
    "# Label the samples (0 for cancer, 1 for non-cancer)\n",
    "combined_dataset['ID_REF'] = np.where(combined_dataset['Disease'] == 'disease state: prostate cancer', 0, 1)\n",
    "\n",
    "combined_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, under_sample_factor=None, over_sample_factor=None):\n",
    "    # Drop metadata columns\n",
    "    columns_to_drop = ['Sample_ID', 'Sex', 'Age', 'Stage', 'Disease']\n",
    "    data = data.drop(columns=columns_to_drop, axis=1)\n",
    "    \n",
    "    x = np.array(data.drop([\"ID_REF\"], axis=1)).astype('float')\n",
    "    y = np.array(data[\"ID_REF\"]).astype('int')\n",
    "    feature_names = data.columns[1:]\n",
    "\n",
    "    if under_sample_factor is not None and isinstance(under_sample_factor, float) and 0 < under_sample_factor <= 1:\n",
    "        under_sampler = RandomUnderSampler(sampling_strategy=under_sample_factor)\n",
    "        x, y = under_sampler.fit_resample(x, y)\n",
    "\n",
    "    if over_sample_factor is not None and isinstance(over_sample_factor, float) and 0 < over_sample_factor <= 1:\n",
    "        over_sampler = RandomOverSampler(sampling_strategy=over_sample_factor)\n",
    "        x, y = over_sampler.fit_resample(x, y)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine model\n",
    "def support_vector_machine(x_train, y_train, feature_num):\n",
    "    pipe = Pipeline([('skb', SelectKBest(f_classif, k=feature_num)), ('estimator', SVC())])\n",
    "\n",
    "    pipe_parameters = {'skb__k': [feature_num],\n",
    "                       'estimator__C': [0.25, 0.5, 0.75, 1],\n",
    "                       'estimator__kernel': ['linear']}\n",
    "\n",
    "    svm_grid_search = GridSearchCV(pipe, pipe_parameters, scoring='accuracy', cv=StratifiedKFold(10), n_jobs=-1)\n",
    "    svm_grid_search.fit(x_train, y_train)\n",
    "\n",
    "    return svm_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SVM metrics\n",
    "def get_svm_metrics(svm_grid_search, x_test, y_test):\n",
    "    best_accuracy = svm_grid_search.best_score_\n",
    "    best_parameters = svm_grid_search.best_params_\n",
    "    print(\"Training Accuracy:\", best_accuracy)\n",
    "    print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "    y_pred = svm_grid_search.predict(x_test)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top SVM features\n",
    "def get_top_svm_features(svm_grid_search, feature_names, top_feature_num):\n",
    "    coef_list = svm_grid_search.best_estimator_.named_steps['estimator'].coef_[0]\n",
    "    features = svm_grid_search.best_estimator_.named_steps['skb'].get_support()\n",
    "    selected_features_list = feature_names[features].tolist()\n",
    "\n",
    "    coef_list, selected_features_list = zip(*sorted(zip(abs(coef_list), selected_features_list), reverse=True))\n",
    "    coef_list, selected_features_list = list(coef_list), list(selected_features_list)\n",
    "\n",
    "    return coef_list[:top_feature_num], selected_features_list[:top_feature_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest model\n",
    "def random_forest(x_train, y_train, feature_num):\n",
    "    pipe = Pipeline([('skb', SelectKBest(f_classif, k=feature_num)), ('estimator', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "    pipe_parameters = {'skb__k': [feature_num],\n",
    "                       'estimator__n_estimators': [100, 500],\n",
    "                       'estimator__max_features': ['sqrt', 'log2'],\n",
    "                       'estimator__criterion': ['gini', 'entropy']}\n",
    "\n",
    "    rf_grid_search = GridSearchCV(pipe, pipe_parameters, scoring='accuracy', cv=StratifiedKFold(10), n_jobs=-1)\n",
    "    rf_grid_search.fit(x_train, y_train)\n",
    "\n",
    "    return rf_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Random Forest metrics\n",
    "def get_rf_metrics(rf_grid_search, x_test, y_test):\n",
    "    best_accuracy = rf_grid_search.best_score_\n",
    "    best_parameters = rf_grid_search.best_params_\n",
    "    print(\"Training Accuracy:\", best_accuracy)\n",
    "    print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "    y_pred = rf_grid_search.predict(x_test)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top Random Forest features\n",
    "def get_top_rf_features(rf_grid_search, feature_names, top_feature_num):\n",
    "    importance_list = rf_grid_search.best_estimator_.named_steps['estimator'].feature_importances_\n",
    "    features = rf_grid_search.best_estimator_.named_steps['skb'].get_support()\n",
    "    selected_features_list = feature_names[features].tolist()\n",
    "\n",
    "    importance_list, selected_features_list = zip(*sorted(zip(importance_list, selected_features_list), reverse=True))\n",
    "    importance_list, selected_features_list = list(importance_list), list(selected_features_list)\n",
    "\n",
    "    return importance_list[:top_feature_num], selected_features_list[:top_feature_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting model\n",
    "def gradient_boosting(x_train, y_train, feature_num):\n",
    "    pipe = Pipeline([('skb', SelectKBest(f_classif, k=feature_num)), ('estimator', GradientBoostingClassifier())])\n",
    "\n",
    "    pipe_parameters = {'skb__k': [feature_num],\n",
    "                       'estimator__learning_rate': [0.5, 1],\n",
    "                       'estimator__n_estimators': [50]}\n",
    "\n",
    "    gb_grid_search = GridSearchCV(pipe, pipe_parameters, scoring='accuracy', cv=StratifiedKFold(10), n_jobs=-1)\n",
    "    gb_grid_search.fit(x_train, y_train)\n",
    "\n",
    "    return gb_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Gradient Boosting metrics\n",
    "def get_gb_metrics(gb_grid_search, x_test, y_test):\n",
    "    best_accuracy = gb_grid_search.best_score_\n",
    "    best_parameters = gb_grid_search.best_params_\n",
    "    print(\"Training Accuracy:\", best_accuracy)\n",
    "    print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "    y_pred = gb_grid_search.predict(x_test)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top Gradient Boosting features\n",
    "def get_top_gb_features(gb_grid_search, feature_names, top_feature_num):\n",
    "    importance_list = gb_grid_search.best_estimator_.named_steps['estimator'].feature_importances_\n",
    "    features = gb_grid_search.best_estimator_.named_steps['skb'].get_support()\n",
    "    selected_features_list = feature_names[features].tolist()\n",
    "\n",
    "    importance_list, selected_features_list = zip(*sorted(zip(importance_list, selected_features_list), reverse=True))\n",
    "    importance_list, selected_features_list = list(importance_list), list(selected_features_list)\n",
    "\n",
    "    return importance_list[:top_feature_num], selected_features_list[:top_feature_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(svm_top_features, rf_top_features, gb_top_features, top_feature_num):\n",
    "    features_list_full = []\n",
    "    features_list_full.extend(svm_top_features)\n",
    "    features_list_full.extend(rf_top_features)\n",
    "    features_list_full.extend(gb_top_features)\n",
    "    features_list = list(dict.fromkeys(features_list_full))\n",
    "\n",
    "    rank_features_list = []\n",
    "    rank_num_list = []\n",
    "\n",
    "    for feature in features_list:\n",
    "        # Check if feature is in each list, use high index if not present to avoid skewing the rank\n",
    "        svm_index = svm_top_features.index(feature) if feature in svm_top_features else len(svm_top_features)\n",
    "        rf_index = rf_top_features.index(feature) if feature in rf_top_features else len(rf_top_features)\n",
    "        gb_index = gb_top_features.index(feature) if feature in gb_top_features else len(gb_top_features)\n",
    "\n",
    "        rank = float(svm_index + rf_index + gb_index) / 3\n",
    "        rank_features_list.append(feature)\n",
    "        rank_num_list.append(rank)\n",
    "\n",
    "    rank_num_list, rank_features_list = zip(*sorted(zip(rank_num_list, rank_features_list)))\n",
    "\n",
    "    rank_features_list = rank_features_list[:top_feature_num]\n",
    "\n",
    "    return rank_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "def create_network(top_features_list, all_features_list, correlation_threshold_factor, cancer_dataset):\n",
    "    cancer_subset, control_subset = cancer_dataset[(cancer_dataset[\"ID_REF\"] == 0)], cancer_dataset[(cancer_dataset[\"ID_REF\"] == 1)]\n",
    "\n",
    "    edges = [((node1, node2), cancer_subset[node1].corr(cancer_subset[node2], method=\"pearson\")) for node1, node2 in itertools.combinations(top_features_list, 2)]\n",
    "    edges = [(node1, node2, {'weight': abs(correlation), 'sign': 1 if correlation > 0 else 0}) for (node1, node2), correlation in edges if abs(correlation) >= correlation_threshold_factor]\n",
    "\n",
    "    nodes = [(feature, {'sides': all_features_list.count(feature) + 1, \"comparison\": 1 if cancer_subset[feature].mean() >= control_subset[feature].mean() else (0 if cancer_subset[feature].mean() < control_subset[feature].mean() else 0.5)}) for feature in top_features_list]\n",
    "\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_edges_from(edges)\n",
    "\n",
    "    degrees = dict(graph.degree())\n",
    "    network_degrees_values, network_degrees_nodes = zip(*sorted(zip(degrees.values(), degrees.keys()), reverse=True))\n",
    "    print(network_degrees_nodes)\n",
    "    print(network_degrees_values)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_bar_charts(top_features_list, full_dataset, path_name):\n",
    "    prostate_cancer = full_dataset[(full_dataset[\"ID_REF\"] == 0)]\n",
    "    no_cancer = full_dataset[(full_dataset[\"ID_REF\"] == 1)]\n",
    "\n",
    "    cancer_dataset = [prostate_cancer, no_cancer]\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    base_path = \"../bar_charts\"\n",
    "    full_path = os.path.join(base_path, path_name)\n",
    "    if not os.path.exists(full_path):\n",
    "        os.makedirs(full_path)\n",
    "\n",
    "    plt.ioff()\n",
    "    for feature_name in top_features_list:\n",
    "        plt.figure(0).clf()\n",
    "        plt.figure(figsize=(9, 6))\n",
    "\n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "        labels = ['Prostate Cancer', 'No Cancer']\n",
    "        means = [data[feature_name].mean() for data in cancer_dataset]\n",
    "        errors = [data[feature_name].sem() * 2 for data in cancer_dataset]\n",
    "        plt.bar(labels, means, yerr=errors, error_kw={'elinewidth': 10, 'ecolor': 'k'}, capsize=15)\n",
    "\n",
    "        plt.title(feature_name)\n",
    "        plt.ylabel('Signal Value')\n",
    "        plt.savefig(os.path.join(full_path, f\"{feature_name}.png\"), dpi=200, bbox_inches='tight')\n",
    "    plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "feature_selection_num = 500\n",
    "feature_importance_num = 10\n",
    "\n",
    "# Process data\n",
    "x_train_nc, x_test_nc, y_train_nc, y_test_nc, feature_names_nc = process_data(combined_dataset, under_sample_factor=None, over_sample_factor=None)\n",
    "\n",
    "# Train and evaluate SVM model\n",
    "svm_grid_search = support_vector_machine(x_train_nc, y_train_nc, feature_selection_num)\n",
    "get_svm_metrics(svm_grid_search, x_test_nc, y_test_nc)\n",
    "svm_top_coef, svm_top_features = get_top_svm_features(svm_grid_search, feature_names_nc, feature_importance_num)\n",
    "print(svm_top_coef)\n",
    "\n",
    "# Train and evaluate Random Forest model\n",
    "rf_grid_search = random_forest(x_train_nc, y_train_nc, feature_selection_num)\n",
    "get_rf_metrics(rf_grid_search, x_test_nc, y_test_nc)\n",
    "rf_top_importance, rf_top_features = get_top_rf_features(rf_grid_search, feature_names_nc, feature_importance_num)\n",
    "print(rf_top_importance)\n",
    "\n",
    "# Train and evaluate Gradient Boosting model\n",
    "gb_grid_search = gradient_boosting(x_train_nc, y_train_nc, feature_selection_num)\n",
    "get_gb_metrics(gb_grid_search, x_test_nc, y_test_nc)\n",
    "gb_top_importance, gb_top_features = get_top_gb_features(gb_grid_search, feature_names_nc, feature_importance_num)\n",
    "print(gb_top_features)\n",
    "\n",
    "# Get top features across models\n",
    "top_features = get_top_features(svm_top_features, rf_top_features, gb_top_features, feature_importance_num)\n",
    "print(top_features, len(top_features))\n",
    "\n",
    "# Analysis and visualization\n",
    "network_graph = create_network(top_features, list(feature_names_nc), correlation_threshold_factor=0.5, cancer_dataset=combined_dataset)\n",
    "# create_bar_charts(top_features, combined_dataset, path_name=\"Feature_Bar_Charts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(svm_grid_search.best_estimator_, x_train_nc, y_train_nc, cv=StratifiedKFold(10))\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean Cross-Validation Score:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Plot learning curve for the best SVM model\n",
    "plot_learning_curve(svm_grid_search.best_estimator_, \"Learning Curve (SVM)\", x_train_nc, y_train_nc, cv=StratifiedKFold(10))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
