{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5643, 2570)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctl_samples = pd.read_csv('../data/control/control_samples.csv')\n",
    "ctl_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1027, 2570)\n"
     ]
    }
   ],
   "source": [
    "cancer_samples = pd.read_csv('../data/cancer/prostate_cancer_samples.csv')\n",
    "print(cancer_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>hsa-miR-28-3p</th>\n",
       "      <th>hsa-miR-27a-5p</th>\n",
       "      <th>hsa-miR-518b</th>\n",
       "      <th>hsa-miR-520b</th>\n",
       "      <th>hsa-miR-498</th>\n",
       "      <th>hsa-miR-512-3p</th>\n",
       "      <th>hsa-miR-491-5p</th>\n",
       "      <th>hsa-miR-490-3p</th>\n",
       "      <th>hsa-miR-452-5p</th>\n",
       "      <th>...</th>\n",
       "      <th>hsa-miR-6881-5p</th>\n",
       "      <th>hsa-miR-6880-3p</th>\n",
       "      <th>hsa-miR-6873-5p</th>\n",
       "      <th>hsa-miR-6872-3p</th>\n",
       "      <th>hsa-miR-6865-5p</th>\n",
       "      <th>hsa-miR-6864-3p</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHC672</td>\n",
       "      <td>-3.396181</td>\n",
       "      <td>-3.396181</td>\n",
       "      <td>1.347562</td>\n",
       "      <td>-3.396181</td>\n",
       "      <td>5.840556</td>\n",
       "      <td>0.486324</td>\n",
       "      <td>5.268448</td>\n",
       "      <td>2.210799</td>\n",
       "      <td>-3.396181</td>\n",
       "      <td>...</td>\n",
       "      <td>1.639051</td>\n",
       "      <td>5.446718</td>\n",
       "      <td>-3.396181</td>\n",
       "      <td>5.004431</td>\n",
       "      <td>4.415208</td>\n",
       "      <td>2.881128</td>\n",
       "      <td>Sex: Male</td>\n",
       "      <td>age: 79</td>\n",
       "      <td>Stage: NA</td>\n",
       "      <td>disease state: no cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHC673</td>\n",
       "      <td>0.821318</td>\n",
       "      <td>2.857542</td>\n",
       "      <td>3.670470</td>\n",
       "      <td>-2.074762</td>\n",
       "      <td>6.143286</td>\n",
       "      <td>2.378031</td>\n",
       "      <td>6.037746</td>\n",
       "      <td>3.703771</td>\n",
       "      <td>1.438320</td>\n",
       "      <td>...</td>\n",
       "      <td>1.731415</td>\n",
       "      <td>6.223110</td>\n",
       "      <td>-2.074762</td>\n",
       "      <td>5.536943</td>\n",
       "      <td>6.122610</td>\n",
       "      <td>4.933096</td>\n",
       "      <td>Sex: Male</td>\n",
       "      <td>age: 66</td>\n",
       "      <td>Stage: NA</td>\n",
       "      <td>disease state: no cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHC681</td>\n",
       "      <td>2.180237</td>\n",
       "      <td>1.718978</td>\n",
       "      <td>4.354315</td>\n",
       "      <td>4.258323</td>\n",
       "      <td>6.214003</td>\n",
       "      <td>-3.339112</td>\n",
       "      <td>5.673198</td>\n",
       "      <td>2.663074</td>\n",
       "      <td>0.651702</td>\n",
       "      <td>...</td>\n",
       "      <td>2.271011</td>\n",
       "      <td>5.580478</td>\n",
       "      <td>-3.339112</td>\n",
       "      <td>4.750161</td>\n",
       "      <td>5.703635</td>\n",
       "      <td>2.825348</td>\n",
       "      <td>Sex: Male</td>\n",
       "      <td>age: 65</td>\n",
       "      <td>Stage: NA</td>\n",
       "      <td>disease state: no cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHC684</td>\n",
       "      <td>-2.203483</td>\n",
       "      <td>-2.203483</td>\n",
       "      <td>3.419279</td>\n",
       "      <td>-2.203483</td>\n",
       "      <td>6.015818</td>\n",
       "      <td>-2.203483</td>\n",
       "      <td>6.504094</td>\n",
       "      <td>-2.203483</td>\n",
       "      <td>1.275758</td>\n",
       "      <td>...</td>\n",
       "      <td>2.470047</td>\n",
       "      <td>5.677397</td>\n",
       "      <td>-2.203483</td>\n",
       "      <td>4.918763</td>\n",
       "      <td>5.480697</td>\n",
       "      <td>-2.203483</td>\n",
       "      <td>Sex: Male</td>\n",
       "      <td>age: 61</td>\n",
       "      <td>Stage: NA</td>\n",
       "      <td>disease state: no cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHC688</td>\n",
       "      <td>-3.558819</td>\n",
       "      <td>-3.558819</td>\n",
       "      <td>2.783801</td>\n",
       "      <td>0.414324</td>\n",
       "      <td>6.077240</td>\n",
       "      <td>-3.558819</td>\n",
       "      <td>5.821377</td>\n",
       "      <td>3.112312</td>\n",
       "      <td>-1.533649</td>\n",
       "      <td>...</td>\n",
       "      <td>3.052431</td>\n",
       "      <td>5.311382</td>\n",
       "      <td>1.195303</td>\n",
       "      <td>4.862444</td>\n",
       "      <td>4.913932</td>\n",
       "      <td>3.920142</td>\n",
       "      <td>Sex: Female</td>\n",
       "      <td>age: 72</td>\n",
       "      <td>Stage: NA</td>\n",
       "      <td>disease state: no cancer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2570 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sample_ID  hsa-miR-28-3p  hsa-miR-27a-5p  hsa-miR-518b  hsa-miR-520b  \\\n",
       "0    CHC672      -3.396181       -3.396181      1.347562     -3.396181   \n",
       "1    CHC673       0.821318        2.857542      3.670470     -2.074762   \n",
       "2    CHC681       2.180237        1.718978      4.354315      4.258323   \n",
       "3    CHC684      -2.203483       -2.203483      3.419279     -2.203483   \n",
       "4    CHC688      -3.558819       -3.558819      2.783801      0.414324   \n",
       "\n",
       "   hsa-miR-498  hsa-miR-512-3p  hsa-miR-491-5p  hsa-miR-490-3p  \\\n",
       "0     5.840556        0.486324        5.268448        2.210799   \n",
       "1     6.143286        2.378031        6.037746        3.703771   \n",
       "2     6.214003       -3.339112        5.673198        2.663074   \n",
       "3     6.015818       -2.203483        6.504094       -2.203483   \n",
       "4     6.077240       -3.558819        5.821377        3.112312   \n",
       "\n",
       "   hsa-miR-452-5p  ...  hsa-miR-6881-5p  hsa-miR-6880-3p  hsa-miR-6873-5p  \\\n",
       "0       -3.396181  ...         1.639051         5.446718        -3.396181   \n",
       "1        1.438320  ...         1.731415         6.223110        -2.074762   \n",
       "2        0.651702  ...         2.271011         5.580478        -3.339112   \n",
       "3        1.275758  ...         2.470047         5.677397        -2.203483   \n",
       "4       -1.533649  ...         3.052431         5.311382         1.195303   \n",
       "\n",
       "   hsa-miR-6872-3p  hsa-miR-6865-5p  hsa-miR-6864-3p          Sex      Age  \\\n",
       "0         5.004431         4.415208         2.881128    Sex: Male  age: 79   \n",
       "1         5.536943         6.122610         4.933096    Sex: Male  age: 66   \n",
       "2         4.750161         5.703635         2.825348    Sex: Male  age: 65   \n",
       "3         4.918763         5.480697        -2.203483    Sex: Male  age: 61   \n",
       "4         4.862444         4.913932         3.920142  Sex: Female  age: 72   \n",
       "\n",
       "       Stage                   Disease  \n",
       "0  Stage: NA  disease state: no cancer  \n",
       "1  Stage: NA  disease state: no cancer  \n",
       "2  Stage: NA  disease state: no cancer  \n",
       "3  Stage: NA  disease state: no cancer  \n",
       "4  Stage: NA  disease state: no cancer  \n",
       "\n",
       "[5 rows x 2570 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctl_samples.columns = ctl_samples.columns.str.strip()\n",
    "ctl_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Stage: CTL\n",
      "1       Stage: CTL\n",
      "2       Stage: CTL\n",
      "3       Stage: CTL\n",
      "4       Stage: CTL\n",
      "           ...    \n",
      "5638    Stage: CTL\n",
      "5639    Stage: CTL\n",
      "5640    Stage: CTL\n",
      "5641    Stage: CTL\n",
      "5642    Stage: CTL\n",
      "Name: Stage, Length: 5643, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ctl_samples['Stage'] = ctl_samples['Stage'].str.replace('Stage: NA', 'Stage: CTL')\n",
    "\n",
    "# Verify the replacements\n",
    "print(ctl_samples['Stage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = pd.concat([cancer_samples, ctl_samples], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Stage column: ['Stage: 3' 'Stage: 2' 'Stage: NA' 'Stage: 4' 'Stage: 1' 'Stage: CTL']\n",
      "Class distribution in ID_REF column:\n",
      "ID_REF\n",
      "0    5643\n",
      "1    1027\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify and clean the Stage column\n",
    "print(\"Unique values in Stage column:\", combined_dataset['Stage'].unique())\n",
    "combined_dataset['Stage'] = combined_dataset['Stage'].str.strip()\n",
    "combined_dataset['ID_REF'] = np.where(combined_dataset['Stage'] == 'Stage: CTL', 0, 1)\n",
    "combined_dataset['ID_REF'] = np.where(combined_dataset['Stage'] != 'Stage: CTL', 1, combined_dataset['ID_REF'])\n",
    "\n",
    "# Print class distribution to ensure both classes are present\n",
    "print(\"Class distribution in ID_REF column:\")\n",
    "print(combined_dataset['ID_REF'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, under_sample_factor=None, over_sample_factor=None):\n",
    "    columns_to_drop = ['Sample_ID', 'Sex', 'Age', 'Stage', 'Disease']\n",
    "    data = data.drop(columns=columns_to_drop, axis=1)\n",
    "    \n",
    "    # Ensure ID_REF is handled separately\n",
    "    id_ref = data[\"ID_REF\"]\n",
    "    data = data.drop([\"ID_REF\"], axis=1)\n",
    "    \n",
    "    # Convert all remaining columns to numeric, coercing errors to NaN\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Handle NaN values, e.g., fill with the mean of each column\n",
    "    data = data.fillna(data.mean())\n",
    "    \n",
    "    # Combine ID_REF back to the DataFrame\n",
    "    data[\"ID_REF\"] = id_ref\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    x = np.array(data.drop([\"ID_REF\"], axis=1)).astype('float')\n",
    "    y = np.array(data[\"ID_REF\"]).astype('int')\n",
    "    feature_names = data.columns[:-1]\n",
    "\n",
    "    if under_sample_factor is not None and isinstance(under_sample_factor, float) and 0 < under_sample_factor <= 1:\n",
    "        under_sampler = RandomUnderSampler(sampling_strategy=under_sample_factor)\n",
    "        x, y = under_sampler.fit_resample(x, y)\n",
    "\n",
    "    if over_sample_factor is not None and isinstance(over_sample_factor, float) and 0 < over_sample_factor <= 1:\n",
    "        over_sampler = RandomOverSampler(sampling_strategy=over_sample_factor)\n",
    "        x, y = over_sampler.fit_resample(x, y)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/rs13jmts3nbf9xfm917mh9p80000gn/T/ipykernel_77761/2988405077.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"ID_REF\"] = id_ref\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "feature_selection_num = 50\n",
    "feature_importance_num = 10\n",
    "\n",
    "num_cancer_samples = combined_dataset['ID_REF'].value_counts()[1]  # Number of benign samples\n",
    "num_control_samples = combined_dataset['ID_REF'].value_counts()[0]  # Number of control samples\n",
    "under_sample_factor = num_cancer_samples / num_control_samples  # Factor to balance the classes\n",
    "\n",
    "# Process the data with under-sampling\n",
    "x_train, x_test, y_train, y_test, feature_names = process_data(combined_dataset, under_sample_factor=under_sample_factor, over_sample_factor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_objective(trial):\n",
    "    k = feature_selection_num\n",
    "    \n",
    "    C = trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
    "    kernel = 'linear'  # Use only linear kernel for feature extraction\n",
    "    \n",
    "    # Relaxed LassoCV with a wider range of smaller alphas\n",
    "    lasso = SelectFromModel(LassoCV(alphas=np.logspace(-6, -1, 50), cv=5, max_iter=10000, random_state=0))\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('skb', SelectKBest(f_classif, k=k)),\n",
    "        ('lasso', lasso),\n",
    "        ('estimator', SVC(C=C, kernel=kernel, random_state=0))\n",
    "    ])\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    try:\n",
    "        scores = cross_val_score(pipe, x_train, y_train, cv=cv, scoring='accuracy')\n",
    "        return scores.mean()\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "        print(\"Selected features by SelectKBest:\", skb.get_support(indices=True))\n",
    "        if hasattr(lasso.estimator_, 'coef_'):\n",
    "            print(\"Lasso coefficients:\", lasso.estimator_.coef_)\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_objective(trial):\n",
    "    k = feature_selection_num\n",
    "    \n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    \n",
    "    lasso = SelectFromModel(LassoCV(alphas=np.logspace(-6, -1, 50), cv=5, max_iter=10000, random_state=0))\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('skb', SelectKBest(f_classif, k=k)),\n",
    "        ('lasso', lasso),\n",
    "        ('estimator', RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, criterion=criterion, random_state=0))\n",
    "    ])\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(pipe, x_train, y_train, cv=cv, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_objective(trial):\n",
    "    k = feature_selection_num\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 1.0, log=True)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 10)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    \n",
    "    lasso = SelectFromModel(LassoCV(alphas=np.logspace(-6, -1, 50), cv=5, max_iter=10000, random_state=0))\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('skb', SelectKBest(f_classif, k=k)),\n",
    "        ('lasso', lasso),\n",
    "        ('estimator', XGBClassifier(learning_rate=learning_rate, max_depth=max_depth, n_estimators=n_estimators, random_state=0, use_label_encoder=False, eval_metric='logloss'))\n",
    "    ])\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(pipe, x_train, y_train, cv=cv, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save study\n",
    "def save_study(study, filename):\n",
    "    joblib.dump(study, filename)\n",
    "\n",
    "# Function to load study\n",
    "def load_study(filename):\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize hyperparameters using Optuna\n",
    "svm_study_filename = '50_svm_study_ctl_c.pkl'\n",
    "rf_study_filename = '50_rf_study_ctl_c.pkl'\n",
    "xgboost_study_filename = '50_xgboost_study_ctl_c.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize hyperparameters using Optuna with early stopping\n",
    "def optimize_with_early_stopping(objective, study_filename, n_trials=50, patience=10):\n",
    "    if os.path.exists(study_filename):\n",
    "        study = load_study(study_filename)\n",
    "        return study\n",
    "    else:\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    best_value = -np.inf\n",
    "    trials_without_improvement = 0\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        study.optimize(objective, n_trials=1)\n",
    "        \n",
    "        current_best_value = study.best_value\n",
    "        if current_best_value > best_value:\n",
    "            best_value = current_best_value\n",
    "            trials_without_improvement = 0\n",
    "        else:\n",
    "            trials_without_improvement += 1\n",
    "        \n",
    "        if trials_without_improvement >= patience:\n",
    "            print(f\"Early stopping at trial {trial + 1}\")\n",
    "            break\n",
    "        \n",
    "        save_study(study, study_filename)\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Hyperparmeters if not trained already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_study = optimize_with_early_stopping(svm_objective, svm_study_filename, n_trials=50, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_study = optimize_with_early_stopping(rf_objective, rf_study_filename, n_trials=100, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_study = optimize_with_early_stopping(xgboost_objective, xgboost_study_filename, n_trials=100, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the best trial for each study\n",
    "\n",
    "For SVM, RF, and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM trial:\n",
      "  Value:  0.9996251171508904\n",
      "  Params: \n",
      "    C: 10.832689243478363\n"
     ]
    }
   ],
   "source": [
    "print(\"Best SVM trial:\")\n",
    "svm_trial = svm_study.best_trial\n",
    "print(\"  Value: \", svm_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in svm_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest trial:\n",
      "  Value:  0.9996251171508904\n",
      "  Params: \n",
      "    n_estimators: 272\n",
      "    max_depth: 26\n",
      "    max_features: sqrt\n",
      "    criterion: gini\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Random Forest trial:\")\n",
    "rf_trial = rf_study.best_trial\n",
    "print(\"  Value: \", rf_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in rf_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost trial:\n",
      "  Value:  0.9996251171508904\n",
      "  Params: \n",
      "    learning_rate: 0.01828145650307612\n",
      "    max_depth: 6\n",
      "    n_estimators: 917\n"
     ]
    }
   ],
   "source": [
    "print(\"Best XGBoost trial:\")\n",
    "xgboost_trial = xgboost_study.best_trial\n",
    "print(\"  Value: \", xgboost_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in xgboost_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the models with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the models with the best hyperparameters\n",
    "def train_and_evaluate(pipe, x_train, y_train, x_test, y_test):\n",
    "    pipe.fit(x_train, y_train)\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    print(f'Testing accuracy {accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Confusion matrix: \\n{confusion_matrix(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.9992503748125937\n",
      "Confusion matrix: \n",
      "[[1128    1]\n",
      " [   0  205]]\n"
     ]
    }
   ],
   "source": [
    "best_svm_params = svm_trial.params\n",
    "svm_pipe = Pipeline([\n",
    "    ('skb', SelectKBest(f_classif, k=feature_selection_num)),\n",
    "    ('lasso', SelectFromModel(LassoCV(alphas=np.logspace(-6, -1, 50), cv=5, max_iter=10000, random_state=0))),\n",
    "    ('estimator', SVC(C=best_svm_params['C'], kernel='linear', random_state=0))  # Ensure linear kernel\n",
    "])\n",
    "train_and_evaluate(svm_pipe, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_accuracies = []\n",
    "\n",
    "# # Loop through the number of features from 1 to 150\n",
    "# for k in range(1, 121):\n",
    "#     best_svm_params = svm_trial.params\n",
    "#     svm_pipe = Pipeline([\n",
    "#         ('skb', SelectKBest(f_classif, k=k)),\n",
    "#         ('lasso', SelectFromModel(LassoCV(cv=5, random_state=0))),\n",
    "#         ('estimator', SVC(C=best_svm_params['C'], kernel=best_svm_params['kernel'], random_state=0))\n",
    "#     ])\n",
    "    \n",
    "#     # Train and evaluate the pipeline\n",
    "#     svm_pipe.fit(x_train, y_train)\n",
    "#     y_pred = svm_pipe.predict(x_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     svm_accuracies.append(accuracy)\n",
    "#     print(f'Number of features: {k}, Accuracy: {accuracy}')\n",
    "\n",
    "# # Plot the number of features vs accuracy\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, 121), svm_accuracies, marker='o')\n",
    "# plt.title('Accuracy vs Number of Features (SVM)')\n",
    "# plt.xlabel('Number of Features')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.9992503748125937\n",
      "Confusion matrix: \n",
      "[[1128    1]\n",
      " [   0  205]]\n"
     ]
    }
   ],
   "source": [
    "best_rf_params = rf_trial.params\n",
    "rf_pipe = Pipeline([\n",
    "    ('skb', SelectKBest(f_classif, k=feature_selection_num)),\n",
    "    ('lasso', SelectFromModel(LassoCV(alphas=np.logspace(-6, -1, 50), cv=5, max_iter=10000, random_state=0))),\n",
    "    ('estimator', RandomForestClassifier(n_estimators=best_rf_params['n_estimators'],\n",
    "                                         max_depth=best_rf_params['max_depth'],\n",
    "                                         max_features=best_rf_params['max_features'],\n",
    "                                         criterion=best_rf_params['criterion'],\n",
    "                                         random_state=0))\n",
    "])\n",
    "train_and_evaluate(rf_pipe, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_accuracies = []\n",
    "\n",
    "# # Loop through the number of features from 1 to 200\n",
    "# for k in range(1, 121):\n",
    "#     best_rf_params = rf_trial.params\n",
    "#     rf_pipe = Pipeline([\n",
    "#         ('skb', SelectKBest(f_classif, k=k)),\n",
    "#         ('lasso', SelectFromModel(LassoCV(cv=5, random_state=0))),\n",
    "#         ('estimator', RandomForestClassifier(n_estimators=best_rf_params['n_estimators'],\n",
    "#                                              max_depth=best_rf_params['max_depth'],\n",
    "#                                              max_features=best_rf_params['max_features'],\n",
    "#                                              criterion=best_rf_params['criterion'],\n",
    "#                                              random_state=0))\n",
    "#     ])\n",
    "    \n",
    "#     # Train and evaluate the pipeline\n",
    "#     rf_pipe.fit(x_train, y_train)\n",
    "#     y_pred = rf_pipe.predict(x_test)\n",
    "#     rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "#     rf_accuracies.append(rf_accuracy)\n",
    "#     print(f'Number of features: {k}, Accuracy: {rf_accuracy}')\n",
    "\n",
    "# # Plot the number of features vs rf_accuracy\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, 121), rf_accuracies, marker='o')\n",
    "# plt.title('Accuracy vs Number of Features')\n",
    "# plt.xlabel('Number of Features')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmetthintz/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.838e-01, tolerance: 6.954e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 1.0\n",
      "Confusion matrix: \n",
      "[[1129    0]\n",
      " [   0  205]]\n"
     ]
    }
   ],
   "source": [
    "best_xgboost_params = xgboost_trial.params\n",
    "xgboost_pipe = Pipeline([\n",
    "    ('skb', SelectKBest(f_classif, k=feature_selection_num)),\n",
    "    ('lasso', SelectFromModel(LassoCV(cv=5, random_state=0))),\n",
    "    ('estimator', XGBClassifier(learning_rate=best_xgboost_params['learning_rate'],\n",
    "                                max_depth=best_xgboost_params['max_depth'],\n",
    "                                n_estimators=best_xgboost_params['n_estimators'],\n",
    "                                random_state=0,\n",
    "                                use_label_encoder=False,\n",
    "                                eval_metric='logloss'))\n",
    "])\n",
    "train_and_evaluate(xgboost_pipe, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost_accuracies = []\n",
    "\n",
    "# # Loop through the number of features from 1 to 150\n",
    "# for k in range(1, 121):\n",
    "#     best_xgboost_params = xgboost_trial.params\n",
    "#     xgboost_pipe = Pipeline([\n",
    "#         ('skb', SelectKBest(f_classif, k=k)),\n",
    "#         ('lasso', SelectFromModel(LassoCV(cv=5, random_state=0))),\n",
    "#         ('estimator', XGBClassifier(learning_rate=best_xgboost_params['learning_rate'],\n",
    "#                                     max_depth=best_xgboost_params['max_depth'],\n",
    "#                                     n_estimators=best_xgboost_params['n_estimators'],\n",
    "#                                     random_state=0,\n",
    "#                                     use_label_encoder=False,\n",
    "#                                     eval_metric='logloss'))\n",
    "#     ])\n",
    "\n",
    "#     # Train and evaluate the pipeline\n",
    "#     xgboost_pipe.fit(x_train, y_train)\n",
    "#     y_pred = xgboost_pipe.predict(x_test)\n",
    "#     xgboost_accuracy = accuracy_score(y_test, y_pred)\n",
    "#     xgboost_accuracies.append(xgboost_accuracy)\n",
    "#     print(f'Number of features: {k}, Accuracy: {xgboost_accuracy}') \n",
    "\n",
    "# # Plot the number of features vs xgboost_accuracy\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(range(1, 121), xgboost_accuracies, marker='o')\n",
    "# plt.title('Accuracy vs Number of Features (XGBoost)')\n",
    "# plt.xlabel('Number of Features')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(pipe, feature_names, top_feature_num):\n",
    "    if isinstance(pipe.named_steps['estimator'], SVC):\n",
    "        if pipe.named_steps['estimator'].kernel != 'linear':\n",
    "            raise ValueError(\"Feature importance is not available for non-linear SVM kernels.\")\n",
    "        feature_scores = pipe.named_steps['estimator'].coef_[0]\n",
    "    elif isinstance(pipe.named_steps['estimator'], XGBClassifier):\n",
    "        feature_scores = pipe.named_steps['estimator'].feature_importances_\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported estimator type for feature extraction.\")\n",
    "    \n",
    "    # Get selected features from SelectKBest\n",
    "    skb_support = pipe.named_steps['skb'].get_support(indices=True)\n",
    "    print(\"Selected features from SelectKBest:\", skb_support)\n",
    "    \n",
    "    # Transform the features using SelectKBest\n",
    "    skb_features = pipe.named_steps['skb'].transform(x_train)\n",
    "    \n",
    "    # Get selected features from LassoCV\n",
    "    lasso = LassoCV(alphas=np.logspace(-6, -1, 50), cv=5, max_iter=10000, random_state=0)\n",
    "    lasso.fit(skb_features, y_train)\n",
    "    lasso_support = np.where(lasso.coef_ != 0)[0]\n",
    "    print(\"Selected features from LassoCV:\", lasso_support)\n",
    "    \n",
    "    if len(lasso_support) == 0:\n",
    "        print(\"No features selected after LassoCV.\")\n",
    "        return []\n",
    "    \n",
    "    # Ensure indices match feature_scores\n",
    "    top_indices = np.argsort(np.abs(lasso.coef_[lasso_support]))[::-1][:top_feature_num]\n",
    "    top_features = [(feature_names[skb_support[i]], lasso.coef_[lasso_support[i]]) for i in top_indices]\n",
    "    \n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features from SelectKBest: [  76  153  221  236  388  423  463  590  697  726  739  780  785  821\n",
      "  832  837  905  931  947 1044 1109 1171 1174 1187 1214 1254 1273 1361\n",
      " 1386 1483 1493 1511 1560 1562 1616 1724 1754 1781 1812 2058 2131 2195\n",
      " 2214 2227 2285 2294 2309 2386 2460 2473]\n",
      "Selected features from LassoCV: [ 0  1  2  3  4  5  6  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 30 31 34 35 36 37 38 39 40 41 42 43 44 45 46 47 49]\n",
      "Top SVM features: [('hsa-miR-6746-5p', -0.07996244612460844), ('hsa-miR-642b-3p', 0.05079726486966696), ('hsa-miR-663a', 0.0391250467252818), ('hsa-miR-3917', 0.0364765313137464), ('hsa-miR-8069', -0.036189824095779186), ('hsa-miR-6766-5p', -0.029477473841805397), ('hsa-miR-1247-3p', 0.028917759671351295), ('hsa-miR-4690-5p', -0.02697001353848485), ('hsa-miR-92a-2-5p', 0.026438922793997722), ('hsa-miR-3940-5p', -0.018227966270909154)]\n",
      "Selected features from SelectKBest: [  76  153  221  236  388  423  463  590  697  726  739  780  785  821\n",
      "  832  837  905  931  947 1044 1109 1171 1174 1187 1214 1254 1273 1361\n",
      " 1386 1483 1493 1511 1560 1562 1616 1724 1754 1781 1812 2058 2131 2195\n",
      " 2214 2227 2285 2294 2309 2386 2460 2473]\n",
      "Selected features from LassoCV: [ 0  1  2  3  4  5  6  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 30 31 34 35 36 37 38 39 40 41 42 43 44 45 46 47 49]\n",
      "Top XGBoost features: [('hsa-miR-6746-5p', -0.07996244612460844), ('hsa-miR-642b-3p', 0.05079726486966696), ('hsa-miR-663a', 0.0391250467252818), ('hsa-miR-3917', 0.0364765313137464), ('hsa-miR-8069', -0.036189824095779186), ('hsa-miR-6766-5p', -0.029477473841805397), ('hsa-miR-1247-3p', 0.028917759671351295), ('hsa-miR-4690-5p', -0.02697001353848485), ('hsa-miR-92a-2-5p', 0.026438922793997722), ('hsa-miR-3940-5p', -0.018227966270909154)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    svm_top_features = get_top_features(svm_pipe, feature_names, feature_importance_num)\n",
    "    print(\"Top SVM features:\", svm_top_features)\n",
    "except ValueError as e:\n",
    "    print(\"SVM feature extraction error:\", e)\n",
    "\n",
    "xgboost_top_features = get_top_features(xgboost_pipe, feature_names, feature_importance_num)\n",
    "print(\"Top XGBoost features:\", xgboost_top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_importance(importance_scores):\n",
    "    max_score = max(importance_scores, key=lambda x: abs(x[1]))[1]\n",
    "    min_score = min(importance_scores, key=lambda x: abs(x[1]))[1]\n",
    "    normalized_scores = [(feature, (score - min_score) / (max_score - min_score)) for feature, score in importance_scores]\n",
    "    return normalized_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled normalized top features: [('SVM_hsa-miR-6746-5p', 1.0), ('SVM_hsa-miR-642b-3p', -1.1180985294466679), ('SVM_hsa-miR-663a', -0.929027232951639), ('SVM_hsa-miR-3917', -0.8861255122631041), ('SVM_hsa-miR-8069', 0.2909534164284971), ('SVM_hsa-miR-6766-5p', 0.1822240601614487), ('SVM_hsa-miR-1247-3p', -0.7636854810146321), ('SVM_hsa-miR-4690-5p', 0.14160720699830845), ('SVM_hsa-miR-92a-2-5p', -0.7235322816481189), ('SVM_hsa-miR-3940-5p', -0.0), ('XGB_hsa-miR-6746-5p', 1.0), ('XGB_hsa-miR-642b-3p', -1.1180985294466679), ('XGB_hsa-miR-663a', -0.929027232951639), ('XGB_hsa-miR-3917', -0.8861255122631041), ('XGB_hsa-miR-8069', 0.2909534164284971), ('XGB_hsa-miR-6766-5p', 0.1822240601614487), ('XGB_hsa-miR-1247-3p', -0.7636854810146321), ('XGB_hsa-miR-4690-5p', 0.14160720699830845), ('XGB_hsa-miR-92a-2-5p', -0.7235322816481189), ('XGB_hsa-miR-3940-5p', -0.0)]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the top features for SVM and XGBoost\n",
    "normalized_svm_features = normalize_importance(svm_top_features)\n",
    "normalized_xgboost_features = normalize_importance(xgboost_top_features)\n",
    "\n",
    "# Compile normalized top features\n",
    "def compile_normalized_top_features(normalized_svm_features, normalized_xgboost_features):\n",
    "    top_features = []\n",
    "    for feature, score in normalized_svm_features:\n",
    "        top_features.append((f'SVM_{feature}', score))\n",
    "    for feature, score in normalized_xgboost_features:\n",
    "        top_features.append((f'XGB_{feature}', score))\n",
    "    return top_features\n",
    "\n",
    "normalized_top_features = compile_normalized_top_features(normalized_svm_features, normalized_xgboost_features)\n",
    "print(\"Compiled normalized top features:\", normalized_top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled normalized top features saved to '../GSEA/miRNA/ctl_c_miRNA.csv'\n"
     ]
    }
   ],
   "source": [
    "normalized_top_features_df = pd.DataFrame(normalized_top_features, columns=['Feature', 'Normalized Importance'])\n",
    "normalized_top_features_df.to_csv('../GSEA/miRNA/ctl_c_miRNA.csv', index=False)\n",
    "print(\"Compiled normalized top features saved to '../GSEA/miRNA/ctl_c_miRNA.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_selected_features(pipe, feature_names, classifier_name):\n",
    "    if isinstance(pipe.named_steps['estimator'], SVC):\n",
    "        if pipe.named_steps['estimator'].kernel != 'linear':\n",
    "            raise ValueError(\"Feature importance is not available for non-linear SVM kernels.\")\n",
    "        feature_scores = pipe.named_steps['estimator'].coef_[0]\n",
    "    elif isinstance(pipe.named_steps['estimator'], XGBClassifier):\n",
    "        feature_scores = pipe.named_steps['estimator'].feature_importances_\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported estimator type for feature extraction.\")\n",
    "    \n",
    "    # Get selected features from SelectKBest\n",
    "    skb_support = pipe.named_steps['skb'].get_support(indices=True)\n",
    "    print(\"Selected features from SelectKBest:\", skb_support)\n",
    "    \n",
    "    # Transform the features using SelectKBest\n",
    "    skb_features = pipe.named_steps['skb'].transform(x_train)\n",
    "    \n",
    "    # Get selected features from LassoCV\n",
    "    lasso = LassoCV(alphas=np.logspace(-6, -1, 50), cv=5, max_iter=10000, random_state=0)\n",
    "    lasso.fit(skb_features, y_train)\n",
    "    lasso_support = np.where(lasso.coef_ != 0)[0]\n",
    "    print(\"Selected features from LassoCV:\", lasso_support)\n",
    "    \n",
    "    # Map the selected LassoCV features back to the original feature names\n",
    "    lasso_support_mapped = skb_support[lasso_support]\n",
    "    \n",
    "    # Ensure the index does not go out of bounds\n",
    "    all_features = [(feature_names[i], feature_scores[j], classifier_name) for j, i in enumerate(lasso_support_mapped) if j < len(feature_scores)]\n",
    "    \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features from SelectKBest: [  76  153  221  236  388  423  463  590  697  726  739  780  785  821\n",
      "  832  837  905  931  947 1044 1109 1171 1174 1187 1214 1254 1273 1361\n",
      " 1386 1483 1493 1511 1560 1562 1616 1724 1754 1781 1812 2058 2131 2195\n",
      " 2214 2227 2285 2294 2309 2386 2460 2473]\n",
      "Selected features from LassoCV: [ 0  1  2  3  4  5  6  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 30 31 34 35 36 37 38 39 40 41 42 43 44 45 46 47 49]\n",
      "All SVM features: [('hsa-miR-8073', 0.1569028212563373, 'SVM'), ('hsa-miR-6861-5p', -0.11564296147886538, 'SVM'), ('hsa-miR-5100', 0.029176992293050236, 'SVM'), ('hsa-miR-8069', -0.16387717170145616, 'SVM'), ('hsa-miR-6802-5p', -2.6407980588486332e-05, 'SVM'), ('hsa-miR-1307-3p', 0.11646436380282953, 'SVM'), ('hsa-miR-6729-5p', 0.13430249137359107, 'SVM'), ('hsa-miR-6131', 0.20900271643512902, 'SVM'), ('hsa-miR-320a', -0.04102615483198485, 'SVM'), ('hsa-miR-614', 0.08277348089859594, 'SVM'), ('hsa-miR-1247-3p', -0.006702252832956896, 'SVM'), ('hsa-miR-663a', 0.11577090145945956, 'SVM'), ('hsa-miR-1228-5p', -0.2268302004164604, 'SVM'), ('hsa-miR-642b-3p', 0.14023832356242252, 'SVM'), ('hsa-miR-1260b', -0.031402287712367694, 'SVM'), ('hsa-miR-4532', 0.14888787892777514, 'SVM'), ('hsa-miR-6756-5p', -0.1875346387670589, 'SVM'), ('hsa-miR-575', -0.09446440191077181, 'SVM'), ('hsa-miR-6746-5p', 0.19863176482134945, 'SVM'), ('hsa-miR-4690-5p', 0.151007215484958, 'SVM'), ('hsa-miR-4730', -0.24391774621360499, 'SVM'), ('hsa-miR-6784-5p', -0.15753217164833444, 'SVM'), ('hsa-miR-92b-5p', 0.033163921791143254, 'SVM'), ('hsa-miR-4687-5p', -0.03135006027907346, 'SVM'), ('hsa-miR-1238-5p', 0.06802418103760671, 'SVM'), ('hsa-miR-8059', -0.036609569503591365, 'SVM'), ('hsa-miR-1469', 0.1429689359290614, 'SVM'), ('hsa-miR-4758-5p', 0.14120886387251697, 'SVM'), ('hsa-miR-6805-5p', -0.15765223330259476, 'SVM'), ('hsa-miR-1203', -0.18078002956833017, 'SVM'), ('hsa-miR-4706', 0.06030927431170448, 'SVM'), ('hsa-miR-197-5p', 0.05641922319473659, 'SVM'), ('hsa-miR-92a-2-5p', -0.19072063175897727, 'SVM'), ('hsa-miR-3940-5p', -0.037585923551589365, 'SVM'), ('hsa-miR-1233-5p', 0.007923605311934319, 'SVM'), ('hsa-miR-4734', 0.20617087882014942, 'SVM'), ('hsa-miR-3917', -0.23844210897576118, 'SVM'), ('hsa-miR-4783-3p', 0.13997828342176455, 'SVM'), ('hsa-miR-6765-5p', -0.15404820132339297, 'SVM'), ('hsa-miR-1343-3p', 0.11749452679872872, 'SVM'), ('hsa-miR-4419b', -0.033871507714878124, 'SVM'), ('hsa-miR-6787-5p', -0.036616070261541095, 'SVM'), ('hsa-miR-4732-5p', -0.13345372442546377, 'SVM'), ('hsa-miR-6769a-5p', -0.06358908875706545, 'SVM')]\n",
      "Selected features from SelectKBest: [  76  153  221  236  388  423  463  590  697  726  739  780  785  821\n",
      "  832  837  905  931  947 1044 1109 1171 1174 1187 1214 1254 1273 1361\n",
      " 1386 1483 1493 1511 1560 1562 1616 1724 1754 1781 1812 2058 2131 2195\n",
      " 2214 2227 2285 2294 2309 2386 2460 2473]\n",
      "Selected features from LassoCV: [ 0  1  2  3  4  5  6  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 30 31 34 35 36 37 38 39 40 41 42 43 44 45 46 47 49]\n",
      "All XGBoost features: [('hsa-miR-8073', 0.246442, 'XGBoost'), ('hsa-miR-6861-5p', 0.0, 'XGBoost'), ('hsa-miR-5100', 0.00094038225, 'XGBoost'), ('hsa-miR-8069', 0.0020274268, 'XGBoost'), ('hsa-miR-6802-5p', 0.24300407, 'XGBoost'), ('hsa-miR-1307-3p', 0.0002507527, 'XGBoost'), ('hsa-miR-6729-5p', 0.00061011687, 'XGBoost'), ('hsa-miR-6131', 0.0011475108, 'XGBoost'), ('hsa-miR-320a', 0.0002442302, 'XGBoost'), ('hsa-miR-614', 0.0, 'XGBoost'), ('hsa-miR-1247-3p', 0.0022151961, 'XGBoost'), ('hsa-miR-663a', 0.4022867, 'XGBoost'), ('hsa-miR-1228-5p', 0.0015682345, 'XGBoost'), ('hsa-miR-642b-3p', 0.0009261506, 'XGBoost'), ('hsa-miR-1260b', 0.008246348, 'XGBoost'), ('hsa-miR-4532', 0.0, 'XGBoost'), ('hsa-miR-6756-5p', 0.0010529088, 'XGBoost'), ('hsa-miR-575', 0.00024564803, 'XGBoost'), ('hsa-miR-6746-5p', 0.0006938964, 'XGBoost'), ('hsa-miR-4690-5p', 0.010897464, 'XGBoost'), ('hsa-miR-4730', 0.0064835916, 'XGBoost'), ('hsa-miR-6784-5p', 8.125279e-05, 'XGBoost'), ('hsa-miR-92b-5p', 0.0, 'XGBoost'), ('hsa-miR-4687-5p', 0.008878441, 'XGBoost'), ('hsa-miR-1238-5p', 0.0025432433, 'XGBoost'), ('hsa-miR-8059', 0.0006476068, 'XGBoost'), ('hsa-miR-1469', 0.0005533064, 'XGBoost'), ('hsa-miR-4758-5p', 0.003575144, 'XGBoost'), ('hsa-miR-6805-5p', 0.0012047042, 'XGBoost'), ('hsa-miR-1203', 0.00041754343, 'XGBoost'), ('hsa-miR-4706', 0.00037503132, 'XGBoost'), ('hsa-miR-197-5p', 0.0003655251, 'XGBoost'), ('hsa-miR-92a-2-5p', 0.0030356685, 'XGBoost'), ('hsa-miR-3940-5p', 0.00039968014, 'XGBoost'), ('hsa-miR-1233-5p', 0.0, 'XGBoost'), ('hsa-miR-4734', 0.046255425, 'XGBoost'), ('hsa-miR-3917', 0.00091016834, 'XGBoost'), ('hsa-miR-4783-3p', 0.00021131073, 'XGBoost'), ('hsa-miR-6765-5p', 4.8424135e-05, 'XGBoost'), ('hsa-miR-1343-3p', 0.0005041861, 'XGBoost'), ('hsa-miR-4419b', 0.0006106627, 'XGBoost'), ('hsa-miR-6787-5p', 0.00010008549, 'XGBoost')]\n",
      "All selected features saved to '../GSEA/miRNA/50_ctl_c.csv'\n"
     ]
    }
   ],
   "source": [
    "# Extract all selected features from SVM and XGBoost\n",
    "all_svm_features = get_all_selected_features(svm_pipe, feature_names, 'SVM')\n",
    "print(\"All SVM features:\", all_svm_features)\n",
    "\n",
    "all_xgboost_features = get_all_selected_features(xgboost_pipe, feature_names, 'XGBoost')\n",
    "print(\"All XGBoost features:\", all_xgboost_features)\n",
    "\n",
    "# Combine the features and save to CSV\n",
    "all_features_df = pd.DataFrame(all_svm_features + all_xgboost_features, columns=['Feature', 'Importance', 'Classifier'])\n",
    "all_features_df.to_csv('../GSEA/miRNA/50_ctl_c.csv', index=False)\n",
    "print(\"All selected features saved to '../GSEA/miRNA/50_ctl_c.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
